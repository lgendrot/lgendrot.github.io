<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

<!-- generator="WordPress/4.1.10" created="2016-03-19 05:17" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Manic Fringe</title>
	<link>http://manicfringe.com</link>
	<description>Biology, code, and a whole lot of coffee.</description>
	<pubDate>Sat, 19 Mar 2016 05:17:07 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>http://manicfringe.com</wp:base_site_url>
	<wp:base_blog_url>http://manicfringe.com</wp:base_blog_url>

	<wp:author><wp:author_id>1</wp:author_id><wp:author_login>lgendrot</wp:author_login><wp:author_email>lgendrot@gmail.com</wp:author_email><wp:author_display_name><![CDATA[Luc Gendrot]]></wp:author_display_name><wp:author_first_name><![CDATA[Luc]]></wp:author_first_name><wp:author_last_name><![CDATA[Gendrot]]></wp:author_last_name></wp:author>


	<generator>http://wordpress.org/?v=4.1.10</generator>

	<item>
		<title>Hello world!</title>
		<link>http://manicfringe.com/?p=1</link>
		<pubDate>Fri, 02 Jan 2015 21:26:14 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=1</guid>
		<description></description>
		<content:encoded><![CDATA[Welcome to WordPress. This is your first post. Edit or delete it, then start blogging!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1</wp:post_id>
		<wp:post_date>2015-01-02 21:26:14</wp:post_date>
		<wp:post_date_gmt>2015-01-02 21:26:14</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>hello-world</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Misc]]></category>
	</item>
	<item>
		<title>New Site!</title>
		<link>http://manicfringe.com/?p=20</link>
		<pubDate>Fri, 02 Jan 2015 22:57:13 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=20</guid>
		<description></description>
		<content:encoded><![CDATA[Woah! Everything looks different!

If you're a returning visitor, you'll notice that the site has been migrated over to WordPress. WordPress gives a larger amount of freedom to determine what is posted and how it is formatted. If you're a new visitor, stay tuned for posts both old and new!

Posts from the old website can still be found <a href="http://manicfringe.tumblr.com" target="_blank">on tumblr</a>, and some of the articles are going to be migrated over to here, but for the most part this is intended as a blank slate.

If you have any comments (or criticisms) concerning the new layout, feel free to use the contact form to air your grievances.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>20</wp:post_id>
		<wp:post_date>2015-01-02 22:57:13</wp:post_date>
		<wp:post_date_gmt>2015-01-02 22:57:13</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>new-site</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Misc]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_done_all</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>dsq_thread_id</wp:meta_key>
			<wp:meta_value><![CDATA[3384446955]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Employment fears in the post-scarcity age.</title>
		<link>http://manicfringe.com/?p=33</link>
		<pubDate>Sun, 04 Jan 2015 19:00:19 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=33</guid>
		<description></description>
		<content:encoded><![CDATA[<h2 style="text-align: center;"><strong>Are we the luddites of the future?</strong></h2>
&nbsp;
<blockquote>
<ol>
	<li>Technology is replacing many forms of human labor, and will continue to do so</li>
	<li>Technology will eventually replace all human labor.</li>
	<li>Eventually there will be no more jobs for humans.</li>
</ol>
</blockquote>
&nbsp;

These premises are the basic form of the Luddite Fallacy, also known as the theory of Technological Unemployment. But do these premises form a logical conclusion? Can we dream of a future where no human need toil on an assembly line, or report to a horrible boss?

&nbsp;

A discussion on the Luddite Fallacy <a href="http://www.reddit.com/r/Futurology/comments/2r4rwk/a_different_perspective_on_technological/">was had recently on /r/Futurology</a>, a Reddit community for futurism hobbyists that frequently hosts discussions with prominent movers in the futurist community.

&nbsp;

User <a href="http://www.reddit.com/user/liamdev">/u/Liamdev</a> uses some fancy economics talk to show that as robots and AI start to replace workers in our current economy, the laws of supply and demand dictate that new sectors of the economy will open up due to shifting demand. In other words, robots will replace some jobs and humans will move on to different ones.

&nbsp;

Liamdev’s discusses the notion that, as consumers, we don’t necessarily choose to spend our money based solely on the utility of what we’re buying. The quintessential example is, of course, Starbucks (or any other suitably over-hyped brand name). People are paying for the Starbucks brand and everything that surrounds it, not the coffee alone.

&nbsp;

Presumably then, as the rise of AI comes to a head, and prices go down accordingly, people will be free to spend their time and money on other pursuits. Those other pursuits will create new sectors of the economy that can go on to flourish, and include things like art, music, and other creative and intellectual pursuits, including those not created yet. Just because everything becomes massively efficient, doesn’t mean humans won’t find new ways to produce “work” or buy products they don’t need based solely on the brand.

&nbsp;

Post-scarcity is what happens when goods become effectively free due to ubiquity and ease of access, and it leads to the creation of those markets becoming inexorably difficult.

&nbsp;

Who’s to say that the post-scarcity won’t lead to more jobs like the strange phenomenon that is “<a href="http://thesnugglebuddies.com/index.html">professional cuddlers</a>”? Or-more traditionally: what if prostitution of all kinds becomes a staple commodity? There’s any number of activities that society now deems frivolous or not worthy of a market that could one day dominate a post-scarcity economy.

&nbsp;
<h2 style="text-align: center;"><strong>So Technological Unemployment is wrong?</strong></h2>
&nbsp;

There are problems with ignoring the possibility of true Technological Unemployment (TU), however. And many of the users of /r/Futurology were quick to discuss the crux of these problems. A common thread brought up throughout the resulting discussion was that this particular round of automation in our history is fundamentally different.

&nbsp;

<a href="http://www.reddit.com/r/Futurology/comments/2r4rwk/a_different_perspective_on_technological/cnch0cw" target="_blank"><img class="alignnone wp-image-48 size-full" src="http://manicfringe.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-04-at-2.09.26-AM-e1420366270321.png" alt="Screen Shot 2015-01-04 at 2.09.26 AM" width="960" height="248" /></a>

&nbsp;

&nbsp;

For one thing, those who view TU as a fallacy tend to point at historical data as evidence of a bad argument. "TU can’t happen because in the past we’ve mechanized jobs and the unemployment rate hasn’t skyrocketed to 70%". This argument seems less convincing when you take into account Moore’s law, or more accurately the concept of “accelerating change”. Indeed if you look at the graph below, popularized by futurist Ray Kurzweil, you see that in the past our rate of technological growth has been relatively linear. Far from the exponential growth we’re in the nearly in the midst of.

&nbsp;

<a href="http://upload.wikimedia.org/wikipedia/commons/d/df/PPTExponentialGrowthof_Computing.jpg"><img class="aligncenter" src="http://upload.wikimedia.org/wikipedia/commons/d/df/PPTExponentialGrowthof_Computing.jpg" alt="" width="755" height="644" /></a>

&nbsp;

&nbsp;

Along the same lines, there’s no guarantee that given the exponential rate of computational growth, that humans will be able to create demand at a rate that keeps pace with human replacement. What if the robots replace us faster than we can create new jobs?

&nbsp;

It’s not just economists who are interested in the potential pitfalls of expanding technological advancement, either. Big players in the technology game and some of the most prominent futurists are also concerned about the impact AI can have on our economy.

&nbsp;

Addressing students at MIT earlier this year, Elon Musk <a href="http://techcrunch.com/2014/10/26/elon-musk-compares-building-artificial-intelligence-to-summoning-the-demon/">described the advent of AI</a> as “summoning the deamon”, giving it the distinction of our “greatest existential threat”. Musk is the founder of Tesla Motors and SpaceX, two wildly successful technology ventures that have put Musk on the map. Despite his trepidation, Musk recently invested in a company called Vicarious, which works on AI algorithms.

&nbsp;

Larry Page, and Sergey Brin, co-founders of google,discussed "abundance" during an interview with<a href="http://www.forbes.com/sites/ellenhuet/2014/07/07/larry-page-robot-jobs/"> Vinod Khosla at the Khosla Ventures summit</a>. Page indicated that with people being replaced by automation, they should just “work less”. He goes so far as to suggest the concept of job sharing, saying that employers should hire 2 part-time workers instead of one full-time worker. Additionally, <a href="http://www.ft.com/intl/cms/s/2/3173f19e-5fbc-11e4-8c27-00144feabdc0.html#axzz3NpkEAvKc">in an interview with FT magazine</a> Page addressed the prospect of losing jobs to technology directly:

&nbsp;
<blockquote>“You can’t wish away these things from happening, they are going to happen,” says Page. “You’re going to have some very amazing capabilities in the economy. When we have computers that can do more and more jobs, it’s going to change how we think about work. There’s no way around that. You can’t wish it away.”</blockquote>
&nbsp;
<h2 style="text-align: center;"><strong>Can we ignore the singularity?</strong></h2>
&nbsp;

There is far from consensus on the idea of TU among the movers and shakers of the technology world, however. There is at least one futurist who is skeptical of Technological Unemployment.

&nbsp;

<a href="http://en.wikipedia.org/wiki/Ray_Kurzweil">Ray Kurzweil</a>, the prominent inventor and futurist who is known for pioneering the idea of “the law of accelerating returns” is also famous for his theory of “<a href="http://en.wikipedia.org/wiki/The_Singularity_Is_Near">the singularity</a>”: a theoretical point in human history wherein our brains and our machines (which he refers to affectionately as brain-extenders) will cease to be separate entities. Kurzweil imagines a future where our biological brains are augmented, or even completely replaced by digital brains with much greater computing power than our current meaty neuron-blobs.

&nbsp;

Although his theories are still just that—theories—they still raise interesting questions about where our society is headed. With brains that can work with the speed of microprocessors, and the flexibility of human reasoning, will the humans of the future be using their augmented brains for work we already think of as being for computers only?

&nbsp;

Take as an extreme example, a future where individuals can purchase a “second brain”. This second brain has all the capabilities of the fastest computers, but is designed to enhance the functioning of a human’s biological brain via a neural connection of some kind. Humans can suddenly do everything they could previously do on a laptop or smartphone right inside their head. Need to send an email to your friend? Just think about it. Programming a new piece of software? No need to sit down to work at a physical screen because your second brain is great at multitasking, so take a walk or continue writing your memoir at the same time.

&nbsp;

The concept of the singularity brings about the possibility of a completely new class of humans whose capabilities in the workforce are completely different than what we can imagine today. Kurzweil himself has stated as much, saying that the rise of smarter and more technologically fused humans is going to completely change our economy and the types of jobs available.

&nbsp;

“We’re going to increase our skills by making ourselves smarter” <a href="http://radioopensource.org/the-end-of-work/">he said to Christopher Lydon</a> on his podcast <em>Open Source</em>. “It’s not going to be us versus the machines, we’re going to enhance our abilities with technology. That’s been the purpose of technology ever since we’ve started it.”

&nbsp;

Kurzweil directly expresses his skepticism concerning the Luddite Fallacy, stating “The majority of jobs that exist today didn’t exist a quarter century ago” echoing the appeal to history so many proponents of the Luddite Fallacy advocate. The inclusion of Kurzweil’s singularity makes this a radically different argument, however. If humans merge with machines then replacing jobs with machines just means replacing “normal” humans with technologically augmented humans.

&nbsp;

&nbsp;
<h2 style="text-align: center;"><strong>Humans and Computers aren’t incompatible</strong></h2>
&nbsp;

Even if we don’t see the singularity of Kurzweil’s predictions, it’s virtually undeniable that future generations will be even more connected and technologically entrenched than we are now. It’s conceivable that new forms of technology will allow humans to do jobs at the same capacity as computers, and perhaps even better.

&nbsp;

We are already seeing the first baby steps towards the merging of computers and humans in the physically disabled: cochlear implants and the world’s first bionic eye are only the beginning. Advances in brain-computer and even brain-brain interfacing are happening rapidly, and commercial products for neurofeedback and even neurogaming are coming to market. The technology behind these products is only going to get better, and their effects remain to be seen.

&nbsp;

[youtube id="qIdKmzeTb3Y" class="center"]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>33</wp:post_id>
		<wp:post_date>2015-01-04 11:00:19</wp:post_date>
		<wp:post_date_gmt>2015-01-04 19:00:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>employment-fears-in-the-post-scarcity-age</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Misc]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_1159dc10ab614c1da0a8691e76269ff4</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_1dc1f043824e3d04eb34673aefa6c443</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_a6544c444ac0d98caa0ee64dc979e79a</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_7bbf30f66c7ad465254925e830ebc14f</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_f03f208ab833414bcff9db30b247fcd6</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_oembed_474dacae8c5d763847126bc919df5b55</wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_thumbnail_id</wp:meta_key>
			<wp:meta_value><![CDATA[53]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_twitter_user</wp:meta_key>
			<wp:meta_value><![CDATA[@lgendrot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_mess</wp:meta_key>
			<wp:meta_value><![CDATA[Employment fears in the post-scarcity age. Are we the Luddites of the future? http://wp.me/p5yxJp-x]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>dsq_thread_id</wp:meta_key>
			<wp:meta_value><![CDATA[3388702614]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_done_all</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839229</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839241</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Runescape Market Analysis (Part 1): Gathering the Data</title>
		<link>http://manicfringe.com/?p=121</link>
		<pubDate>Tue, 03 Mar 2015 21:58:32 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=121</guid>
		<description></description>
		<content:encoded><![CDATA[I have a soft spot in my heart for Runescape. I spent a lot of time in my Junior High's computer lab casting steely glances around the room as my friends and I endeavored to play without the teacher seeing what we were up to instead of practicing our touch typing (snore!)

It was that fuzzy feeling, coupled with my more recent interest in teaching myself Machine Learning and Python that gave me the inspiration to make a <a href="https://twitter.com/RSMarketWatch">market analysis bot</a> for the Runescape Grand Exchange. The bot gathers the day's data, adds it to a 3+ year long dataset scraped from the web, analyzes the last 20 days, and tweets the items that are projected to increase in price some time in the next 20 days.

I'm not sure if this has been done before. I'm sure it has. But it was certainly a fun learning experience either way.

Before I begin I want to give a very big shout out to Harrison Kinsley (who has no idea who I am) and his <a href="https://www.youtube.com/user/sentdex/featured" target="_blank">YouTube channel.</a> His video series on pattern analysis with Python make up the bulk of the code for the bot, and although I'm hoping to improve its performance someday, it does not by any means run on an original algorithm. Check out Harrison's videos on pattern recognition for a more in-depth explanation of how this all works.


<hr />

Let's start with how I gathered the data.

There are a number of websites out there that provide historical information on the Grand Exchange. I chose <a href="http://www.grandexchangewatch.com/">Grand Exchange Watch</a>, which provides data in the form of a table with dates, and prices from those dates:


<a href="http://manicfringe.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-02-at-10.54.02-PM.png"><img class="aligncenter wp-image-146 size-full" src="http://manicfringe.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-02-at-10.54.02-PM.png" alt="Screen Shot 2015-03-02 at 10.54.02 PM" width="893" height="387" /></a>



Unfortunately there's no way to export data from Grand Exchange Watch as far as I know, so I had to scrape the data myself using <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>

I grabbed a list of item IDs from the runescape website, and headed on over to Grand Exchange Watch where I ran into my first problem. Unlike the official Runescape website, Grand Exchange Watch includes both the item name and the item ID in the URL.



<a href="http://manicfringe.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-28-at-9.09.58-PM.png"><img class="aligncenter wp-image-128 size-full" src="http://manicfringe.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-28-at-9.09.58-PM.png" alt="Screen Shot 2015-02-28 at 9.09.58 PM" width="499" height="35" /></a>



If I'm going to programmatically grab all the data I need, I need to be able to arbitrarily go to any item page on this website. But all I have is a list of item IDs, so I wrote a quick function to use <a href="http://docs.python-requests.org/en/latest/">requests</a> and the <a href="http://services.runescape.com/m=rswiki/en/Grand_Exchange_APIs">Runescape API</a> to grab the item names from their ID numbers:


<pre data-language="python">
import json
import requests
from bs4 import BeautifulSoup
import re
import os
import time
import datetime


item_ids = ['1944', '556', '314']
item_names = {}

#the function accepts my list of ID numbers as strings or ints
def getItemNames(item_numbers):
    #for every itemID in my list grab the appropriate json from the Runescape API
    for item_id in item_numbers:
         item_url = 'http://services.runescape.com/m=itemdb_rs/api/catalogue/detail.json?item='+str(item_id)
         item_response = requests.get(item_url)
         item_json = item_response.json()
 
         #stick the item name and id number in a dictionary
         item_names[str(item_json['item']['id'])] = item_json['item']['name']</pre>

<p>So now I have a way to refer to the item's ID and the item's name.</p>


<pre data-language="python">
url_names = []

getItemNames(item_ids)

for item_id in item_names:
    url_names.append(item_id+ "-" +item_names[item_id].replace(" ", "-"))
    #item_id is the ID Number key from our item_names dictionary,
    #and item_names[item_id] is the name of the item

</pre>


I now have a list of names and IDs to append to my URLs. It's now a matter of grabbing the appropriate dates and prices from the table on each page. Also notice that if I want more than just the most recent 20 days I'm going to have to go through each page of the table, which is just a matter of appending a number to the url. For example page 2 of the table is:

<img class="aligncenter size-full wp-image-152" src="http://manicfringe.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-02-at-11.34.51-PM.png" alt="Screen Shot 2015-03-02 at 11.34.51 PM" width="507" height="29" />



I use Requests to grab the page's source, and I stuck the next part into two big for loops. I'm not sure if this is the most pythonic way to do what I'm trying to do, but it works for me and that's all I really care about.
<pre data-language="python">dates = []
prices = []

pagenums = range(1, 60)

for item_url in url_names:
    item_id = re.split(r'([0-9]*)', item_url)[1]
    for pagenum in pagenums:
        url = "http://www.grandexchangewatch.com/item/" + item_url + "?range=360&amp;start=" + str(pagenum)
        r = requests.get(url)
        soup = BeautifulSoup(r.content)
        cal = soup.find_all("div", {"id": "calendar-container"})
        #finds the table of data

        tds = cal[0].contents[5].find_all("td")
        #finds the individual entries in the table, reading from right to left


        date_id_1 = [x*6 for x in range(10)]
        date_id_2 = [x+3 for x in date_id_1]
        #These weird lists of numbers are the indexes for
        #the appropriate entries in the table (Date, Price)
        price_id_1 = [1+(x*6) for x in range(10)]
        price_id_2 = [x+3 for x in price_id_1]

        for i in date_id_1:
            dates.append(tds[i].text)
        for y in date_id_2:
            dates.append(tds[y].text)

        for i in price_id_1:
            prices.append(tds[i].text.replace(",",'').replace("gp",""))
        for y in price_id_2:
            prices.append(tds[i].text.replace(",",'').replace("gp",""))

        dates.reverse()
        prices.reverse()
        #Want the lists so they are from oldest to newest

    f = open(item_names[item_id]+'.csv', 'a+')
    for date in dates:
        ind = dates.index(date)
        entry = str(time.mktime(time.strptime(date, "%B %d, %Y"))) + "," + prices[ind] + "\n"
        #make the string formatted date into a unix timestamp
        f.write(entry)
    f.close()
    #Saving a csv file of what we've gathered

    dates[:] = []
    prices[:] = []
    #Clearing the date and price lists for the next item in our item_id list
</pre>


Before anyone yells at me because this is bad and unreadable...I know. It's slow, and I should probably stick it in a function.

Frankly, I would appreciate any tips on how to improve this code because I'm fairly certain it's amateurish at best. I'll get around to putting this whole project up on github eventually for people to make improvements on and when I do I'll post a link here and in subsequent posts.

And there we have it. This will grab 60 pages of table data for a list of items, which is a little over 3 years of prices, every day, for each item. In Part 2 I'll go over how I analyze this historical data.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>121</wp:post_id>
		<wp:post_date>2015-03-03 13:58:32</wp:post_date>
		<wp:post_date_gmt>2015-03-03 21:58:32</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>runescape-market-analysis-part-1-gathering-the-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="tech"><![CDATA[Code]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_twitter_user</wp:meta_key>
			<wp:meta_value><![CDATA[@lgendrot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_thumbnail_id</wp:meta_key>
			<wp:meta_value><![CDATA[125]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_done_all</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_mess</wp:meta_key>
			<wp:meta_value><![CDATA[In which I write bad code to do cool things | Runescape Grand Exchange Analysis (Part 1): Gathering the Data http://wp.me/p5yxJp-1X]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>dsq_thread_id</wp:meta_key>
			<wp:meta_value><![CDATA[3564748105]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>1</wp:comment_id>
			<wp:comment_author><![CDATA[Runescape Market Analysis (Part 2): Analyzing the Data | Manic Fringe]]></wp:comment_author>
			<wp:comment_author_email></wp:comment_author_email>
			<wp:comment_author_url>http://manicfringe.com/?p=187</wp:comment_author_url>
			<wp:comment_author_IP>104.219.248.39</wp:comment_author_IP>
			<wp:comment_date>2015-03-05 18:41:23</wp:comment_date>
			<wp:comment_date_gmt>2015-03-06 02:41:23</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Part 1 &#8211; Gathering the Data [&#8230;]]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type>pingback</wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
	<item>
		<title>Runescape Market Analysis (Part 2): Analyzing the Data</title>
		<link>http://manicfringe.com/?p=187</link>
		<pubDate>Fri, 06 Mar 2015 02:41:17 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=187</guid>
		<description></description>
		<content:encoded><![CDATA[This is part 2 of a 3 part series on my attempt to accurately predict future prices of items on the Runescape Grand Exchange.

<a href="http://manicfringe.com/?p=121">Part 1 - Gathering the Data</a>

And in case you didn't see it in part 1, <a href="https://twitter.com/RSMarketWatch" target="_blank">here's the end result of this entire endeavor</a>: A twitter bot that tweets about items projected to increase in price on the Grand Exchange within the next 20 days.

<hr />

&nbsp;

At the end of part 1 I gathered 3 years worth of historical data by scraping Grand Exchange Watch and stuck it in a set of CSV files. Each row of one of the CSV files is 1 unix date stamp, and 1 price of the item on that date. Like so:
<img class="aligncenter wp-image-207 size-full" src="http://manicfringe.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-03-at-7.53.14-PM1-e1425609054333.png" alt="Screen Shot 2015-03-03 at 7.53.14 PM" width="429" height="113" />

Simple enough. So I have this rather long data file, now what?

I need a way to go through this collection and collect patterns of price changes. I think 20 days is quite enough time for a price movement to occur, so I'll make all the patterns 20 points long.  I'll start with an "anchor" point, and iterate through the next 20 price points, taking the percentage change between the anchor point and each point after it, up to 20. At this point I'll move the anchor 1 down the list and do the same. In the end there will be as many patterns as there are data-points, minus 20 (I don't want the most recent 20 days to be included in the pattern collection).

First things first, I need a percentage change function:
<pre data-language="python">def percentChange(startPoint, currentPoint):
    try:
        x = ((currentPoint - float(startPoint)) / abs(startPoint)) * 100
        if x == 0.0:
            return 0.00000000001
        else:
            return x
    except:
        return 0.00000000001</pre>
If there is no change between the two points I'm working with I don't want the function to return exactly 0, as that ends up causing the code to return some errors later on (I'm not entirely sure why). So 0.00000000001 is close enough to 0.0 to work. Getting exactly 0 is rare enough that it's not really an issue anyway.

Okay, so now on to collecting the patterns. The following code does exactly as I described above.
<pre data-language="python">def patternStorage():
    x = len(item_price) - 20

    y = 20
    #I don't want to gather the most recent 20 items, thus if y &lt; x:
    while y &lt; x:
        pattern = []
        patStartTime = time.time()
        
        ## Steps to gathering the pattern ##
        ####################################
        #The following collects 20 points of pattern data
        #1) Starting at y-20 (y starts at 20 so: 20-20 = 0)
        #2) Get the percentChange from y-20 (0) to y-19 (1)
        #   Then the percent change from y-20 (0) to y-18 (2) 
        #   etc.
        #3) Append p1 through p20 to a list (list of lists)
        #4) The outcome range is the NEXT 20 items after y,
        #   The average outcome is the average of those
        #   The performance array is a list of all these averages
        #5) y+1
        #6) Start over with the next Y value (21-20=1)
        #7) Compare it to Y-19 (21-19=2) I.E. Percent change from 1 to 2

        p1 = percentChange(item_price[y-20], item_price[y-19])
        p2 = percentChange(item_price[y-20], item_price[y-18])
        p3 = percentChange(item_price[y-20], item_price[y-17])
        p4 = percentChange(item_price[y-20], item_price[y-16])
        p5 = percentChange(item_price[y-20], item_price[y-15])
        p6 = percentChange(item_price[y-20], item_price[y-14])
        p7 = percentChange(item_price[y-20], item_price[y-13])
        p8 = percentChange(item_price[y-20], item_price[y-12])
        p9 = percentChange(item_price[y-20], item_price[y-11])
        p10 = percentChange(item_price[y-20], item_price[y-10])
        p11 = percentChange(item_price[y-20], item_price[y-9])
        p12 = percentChange(item_price[y-20], item_price[y-8])
        p13 = percentChange(item_price[y-20], item_price[y-7])
        p14 = percentChange(item_price[y-20], item_price[y-6])
        p15 = percentChange(item_price[y-20], item_price[y-5])
        p16 = percentChange(item_price[y-20], item_price[y-4])
        p17 = percentChange(item_price[y-20], item_price[y-3])
        p18 = percentChange(item_price[y-20], item_price[y-2])
        p19 = percentChange(item_price[y-20], item_price[y-1])
        p20 = percentChange(item_price[y-20], item_price[y])

        #How far ahead to look
        outcomeRange = item_price[y+1:y+21]
        currentPoint = item_price[y]

        try:
            #The average % change increase/decrease for the next 20 days
            avgOutcome = reduce(lambda x,y: x+y, outcomeRange/len(outcomeRange))
        except Exception, e:
            print str(e)
            avgOutcome=0

        futureOutcome = percentChange(currentPoint, avgOutcome)

        pattern.append(p1)
        pattern.append(p2)
        pattern.append(p3)
        pattern.append(p4)
        pattern.append(p5)
        pattern.append(p6)
        pattern.append(p7)
        pattern.append(p8)
        pattern.append(p9)
        pattern.append(p10)
        pattern.append(p11)
        pattern.append(p12)
        pattern.append(p13)
        pattern.append(p14)
        pattern.append(p15)
        pattern.append(p16)
        pattern.append(p17)
        pattern.append(p18)
        pattern.append(p19)
        pattern.append(p20)

        patternAr.append(pattern)
        performanceAr.append(futureOutcome)

        y+=1</pre>
<em>A note about the above code: Yes. It <span style="text-decoration: underline;">should</span> be written with a loop or otherwise shortened. The primary reason it is written so repetitively is for the sake of learning and for my own understanding. Writing it like this makes it abundantly clear what the patterns consist of.</em>

I'm also going to need to do the same thing for the most recent pattern (the last 20 items in the CSV). Luckily I only need to do this once, so no for loop this time.
<pre data-language="python">def currentPattern():
    #Starting at index -20 is starting 20 items in the past
    #It goes all the way to index -1, which is the most recent item in the price list


    cp1 = percentChange(item_price[-21], item_price[-20])
    cp2 = percentChange(item_price[-21], item_price[-19])
    cp3 = percentChange(item_price[-21], item_price[-18])
    cp4 = percentChange(item_price[-21], item_price[-17])
    cp5 = percentChange(item_price[-21], item_price[-16])
    cp6 = percentChange(item_price[-21], item_price[-15])
    cp7 = percentChange(item_price[-21], item_price[-14])
    cp8 = percentChange(item_price[-21], item_price[-13])
    cp9 = percentChange(item_price[-21], item_price[-12])
    cp10 = percentChange(item_price[-21], item_price[-11])
    cp11 = percentChange(item_price[-21], item_price[-10])
    cp12 = percentChange(item_price[-21], item_price[-9])
    cp13 = percentChange(item_price[-21], item_price[-8])
    cp14 = percentChange(item_price[-21], item_price[-7])
    cp15 = percentChange(item_price[-21], item_price[-6])
    cp16 = percentChange(item_price[-21], item_price[-5])
    cp17 = percentChange(item_price[-21], item_price[-4])
    cp18 = percentChange(item_price[-21], item_price[-3])
    cp19 = percentChange(item_price[-21], item_price[-2])
    cp20 = percentChange(item_price[-21], item_price[-1])


    patForRec.append(cp1)
    patForRec.append(cp2)
    patForRec.append(cp3)
    patForRec.append(cp4)
    patForRec.append(cp5)
    patForRec.append(cp6)
    patForRec.append(cp7)
    patForRec.append(cp8)
    patForRec.append(cp9)
    patForRec.append(cp10)
    patForRec.append(cp11)
    patForRec.append(cp12)
    patForRec.append(cp13)
    patForRec.append(cp14)
    patForRec.append(cp15)
    patForRec.append(cp16)
    patForRec.append(cp17)
    patForRec.append(cp18)
    patForRec.append(cp19)
    patForRec.append(cp20)</pre>
In case you're wondering why I take the percent change of everything in both of these functions: It's because I have to normalize the patterns. Otherwise if an item used to fluctuate around 100gp, but now fluctuates around 200gp, my algorithm wouldn't pick up on any patterns. There are all sorts of ways to normalize data, and this just happens to be the way that I chose (Or rather that Harrison chose in the YouTube videos I followed).

Next up I have to figure out how similar the current pattern is to all the other patterns in my dataset
<pre data-language="python">def patternRecognition():
 
    predictedOutcomesAr = []
    patFound = 0
    plotPatAr = []
    

    #For each pattern in the full pattern list, check each point in that pattern
    #against the corresponding point in the current pattern
    #Example: if the % change between one point in a historical pattern and the
    #corresponding point in the current pattern is 2% then they are 98% similar
    #at that point
 
    for eachPattern in patternAr:
        sim1 = 100.0 - abs(percentChange(eachPattern[0], patForRec[0]))
        sim2 = 100.0 - abs(percentChange(eachPattern[1], patForRec[1]))
        sim3 = 100.0 - abs(percentChange(eachPattern[2], patForRec[2]))
        sim4 = 100.0 - abs(percentChange(eachPattern[3], patForRec[3]))
        sim5 = 100.0 - abs(percentChange(eachPattern[4], patForRec[4]))
        sim6 = 100.0 - abs(percentChange(eachPattern[5], patForRec[5]))
        sim7 = 100.0 - abs(percentChange(eachPattern[6], patForRec[6]))
        sim8 = 100.0 - abs(percentChange(eachPattern[7], patForRec[7]))
        sim9 = 100.0 - abs(percentChange(eachPattern[8], patForRec[8]))
        sim10 = 100.0 - abs(percentChange(eachPattern[9], patForRec[9]))
        sim11 = 100.0 - abs(percentChange(eachPattern[10], patForRec[10]))
        sim12 = 100.0 - abs(percentChange(eachPattern[11], patForRec[11]))
        sim13 = 100.0 - abs(percentChange(eachPattern[12], patForRec[12]))
        sim14 = 100.0 - abs(percentChange(eachPattern[13], patForRec[13]))
        sim15 = 100.0 - abs(percentChange(eachPattern[14], patForRec[14]))
        sim16 = 100.0 - abs(percentChange(eachPattern[15], patForRec[15]))
        sim17 = 100.0 - abs(percentChange(eachPattern[16], patForRec[16]))
        sim18 = 100.0 - abs(percentChange(eachPattern[17], patForRec[17]))
        sim19 = 100.0 - abs(percentChange(eachPattern[18], patForRec[18]))
        sim20 = 100.0 - abs(percentChange(eachPattern[19], patForRec[19]))
        #Note this is where a some error might appear in the algorithm, as 
        #it's taking a percent change of a percent change. Not great.

        #average of all the sims above
        howSim = (sim1 + sim2 + sim3 + sim4 + sim5 + sim6 + sim7 + sim8 + sim9 + sim10+
 sim11 + sim12 + sim13 + sim14 + sim15 + sim16 + sim17 + sim18 + sim19 + sim20) / 20.0

        if howSim &gt; 75:
            patFound = 1
            xp = range(1, 21)
            similarPatterns.append(eachPattern)

        #As long as one pattern is found that is 75% similar or higher
        if patFound == 1:
            #Take each one of those patterns
            for eachpatt in similarPatterns:
                #Find its index in our complete list of patterns
                futurePoints = patternAr.index(eachpatt)
                #put the corresponding outcome into the predictedOutcomesAr list
                predictedOutcomesAr.append(performanceAr[futurePoints])
                #If it's greater than the current point
                if performanceAr[futurePoints] &gt; patForRec[19]:
                    #Increase positive outcomes by 1
                    positiveOutcomes += 1

 
            #Take the average of all our predicted outcomes
            predictedAvgOutcome = reduce(lambda x, y: x+y, predictedOutcomesAr) / len(predictedOutcomesAr)

            #If on average the outcome will be greater than the current price
            if predictedAvgOutcome &gt; patForRec[19]:
                positive = 1
                OutcomePrediction = predictedAvgOutcome</pre>
With this function I have iterated through all the patterns in our CSV, and I have compared them with the most recent pattern. For all the patterns that are &gt;75% similar, I add them to a list called similarPatterns.

I then average the historical outcomes of each entry in similarPatterns, and if the average is greater than the current price of the item, I label the item as having a net positive prediction.Additionally, I count the number of positive outcomes from the similarPatterns list. This gives me a couple of variables that describe the total number of similar positive outcomes and whether or not the average outcome is predicted to be positive.

And that pretty much completes the analysis. Now I just have to decide if I want to count this particular item as "interesting" or not. And of course I have to call the previous 3 functions in the proper order to do so.
<pre data-language="python">#setting global variables
interestingItems = []
item_names = {}
patternAr = []
performanceAr = []
patForRec = []
positiveOutcomes = 0
positive = 0
OutcomePrediction = 0

#Uses the getItemNames function from part 1
getItemNames(item_ids)

#Runs all the steps of our pattern analysis in the proper order
def Analyze():
    patternStorage()
    currentPattern()
    patternRecognition()
 
    #Determines whether to add an item to our "interesting items" list
    #Also calculates by how many percentage points the price will rise
    if positiveOutcomes &gt; 1 and positive == 1:
        priceChangePercent = int(abs(patForRec[19]-OutcomePrediction))/100.0
        newPrice = ((priceChangePercent)*item_price[-1]) + item_price[-1]
        interestingItems.append(str(item_names[item]) + "," + str(item) + "," + str(item_price[-1]) + "," + str(priceChangePercent) + "," + str(newPrice))

    #Clears global variables
    patternAr[:] = []
    performanceAr[:] = []
    patForRec[:] = []
    positiveOutcomes = 0
    positive = 0

</pre>
This Analyze function will do everything I need to do for each item. I made a function out of it since I'm going to be calling it for every CSV file I gathered in part 1. Like so:
<pre data-language="python">for item in item_names:
    datestamp, gold = np.loadtxt(str(item_names[item])+".csv", unpack=True,
 delimiter=',')
    item_price = gold
    Analyze()</pre>
And finally I create a CSV file that contains all of the items of interest based on the analysis.
<pre data-language="python">f = open('ItemsOfInterest.csv', 'w+')
for item in interestingItems:
	f.write(str(item)+'\n')
f.close()</pre>
And there we have it, a (messy) script that will scan for and save items that might be increasing in price in the next 20 days.

Again, if you have suggestions for improving the code, or questions about how it works go ahead and leave a comment or <a title="Contact Me" href="http://manicfringe.com/?page_id=12" target="_blank">shoot me an email</a>.

In part 3 I discuss how I update the CSV with the current day's price, and how I set up the twitter bot.

Thanks for reading!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>187</wp:post_id>
		<wp:post_date>2015-03-05 18:41:17</wp:post_date>
		<wp:post_date_gmt>2015-03-06 02:41:17</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>runescape-market-analysis-part-2-analyzing-the-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="tech"><![CDATA[Code]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_thumbnail_id</wp:meta_key>
			<wp:meta_value><![CDATA[188]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_twitter_user</wp:meta_key>
			<wp:meta_value><![CDATA[@lgendrot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_done_all</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_mess</wp:meta_key>
			<wp:meta_value><![CDATA[More bad code, and yet more cool things | Runescape Market Analysis (Part 2): Analyzing the Data http://wp.me/p5yxJp-31]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839229</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839241</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>dsq_thread_id</wp:meta_key>
			<wp:meta_value><![CDATA[3571563281]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>4</wp:comment_id>
			<wp:comment_author><![CDATA[Jake]]></wp:comment_author>
			<wp:comment_author_email>jake.medal@gmail.com</wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP>162.204.244.155</wp:comment_author_IP>
			<wp:comment_date>2015-11-09 22:29:00</wp:comment_date>
			<wp:comment_date_gmt>2015-11-10 06:29:00</wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey, could you let me know if you get this up on GitHub? I'd like to make something similar for OSRS :)]]></wp:comment_content>
			<wp:comment_approved>1</wp:comment_approved>
			<wp:comment_type></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
			<wp:commentmeta>
				<wp:meta_key>dsq_parent_post_id</wp:meta_key>
				<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
			<wp:commentmeta>
				<wp:meta_key>dsq_post_id</wp:meta_key>
				<wp:meta_value><![CDATA[2351368915]]></wp:meta_value>
			</wp:commentmeta>
		</wp:comment>
	</item>
	<item>
		<title>Weekend Project: Reddit AMA Bot</title>
		<link>http://manicfringe.com/?p=214</link>
		<pubDate>Sat, 26 Sep 2015 23:24:09 +0000</pubDate>
		<dc:creator><![CDATA[lgendrot]]></dc:creator>
		<guid isPermaLink="false">http://manicfringe.com/?p=214</guid>
		<description></description>
		<content:encoded><![CDATA[<h4 style="text-align: center;"><em>Pictured above: The Inspiration</em></h4>
<h2 style="text-align: center;">The Inspiration</h2>
Reddit user <a href="http://reddit.com/u/Insanimate" target="_blank">/u/Insanimate</a> made this hotsauce packet painting and posted it on <a href="http://reddit.com/r/pics" target="_blank">/r/pics</a>. The president of TacoBell-or whoever runs their social media accounts-then showed up in the post and asked how they could buy one.

<img class="aligncenter  wp-image-215" src="http://manicfringe.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-26-at-3.32.42-PM-e1443306853325.png" alt="Screen Shot 2015-09-26 at 3.32.42 PM" width="749" height="386" />

Nobody noticed. The painting was posted over a year ago, yet the president's comment went unrecognized and unseen at only 13 points before it wound up on /r/bestof last week.

This is a scenario I've seen play out all over reddit whenever a 'celebrity' reddit user makes a comment. Nobody knows, or is skeptical about the veracity of a user's identity.

So to help ameliorate this problem I made a reddit bot that searches through a user's history, and returns a table of the user's AMA posts. If there are no such posts, then nothing happens.
<h2 style="text-align: center;">The Code</h2>
<pre data-language="python">import praw
import re
import OAuth2Util
import pymongo
import datetime
import time

user_agent = 'AMA identifier: v0.2 (by /u/Molag_balls)'
r = praw.Reddit(user_agent)
o = OAuth2Util.OAuth2Util(r, configfile="oauthconfig.ini")
o.refresh(force=True)

#Connect to mongodb
try:
    conn = pymongo.MongoClient()
    print "Connected successfully!"
except pymongo.errors.ConnectionFailure, e:
    print "Could not connect to MongoDB: %s" % e

db = conn.IdentityBot
collection = db.CommentIDs


def in_database(comment_id): #Is the entry in the database already?
    if collection.find({"comment_id": comment_id}).count() &gt; 0:
        return True
    else:
        return False

def already_done_to_db(already_done):
    for id in already_done:
        collection.update_one(
            {"comment_id": id},
            {
                 "$set": 
                     {
                         "collected_at": datetime.datetime.now().strftime("%s") 
                     }
            },
            upsert=True
        )


#Loop 'forever'
while True:

    #Get all comments in subreddit(s) of interest
    print "Getting comments..."
    all_comments = r.get_comments('bottesting', limit=None)
    already_done = []

    print "Searching through comments..."
    #Look through every comment
    for comment in all_comments:
    #If the comment starts with "!identify, and it's not in the database already"
        if comment.body.lower().startswith('!identify') and not in_database(comment.id):
 
    #If no username is present, use the author of the parent comment
    if comment.body.lower() == "!identify":
        parent_comment = r.get_info(thing_id=comment.parent_id)
        parent = r.get_redditor(parent_comment.author)
 
 #Else use the username provided
    elif re.search(r'/u/([A-Za-z0-9_-]*)', comment.body):
        parent = r.get_redditor(re.search(r'/u/([A-Za-z0-9_-]*)', comment.body).group(1))
 
 #Get all of the submissions made by user in question
    parent_submissions = parent.get_submitted('new', 'all', limit=None)
    amas_found = []
    for submission in parent_submissions:
 #Regex expression finds possible AMA posts, no matter the subreddit.
    if re.search(r'((?:i am|iam|iama|we are) .* (:?ama|amaa|ask me almost anything|aua|ausa|ask us anything|ask me anything).*)', str(submission.title.encode('utf-8', 'ignore')).lower()):
 #amas_found will be a list of tuples containing the title, permalink, and number of comments in the AMA in question
        amas_found.append((submission.title.encode('utf-8', 'ignore'), submission.permalink, submission.num_comments))
 
 #Only if we've actually found any AMAs will we do anything
    if amas_found:
        comment_table_header = "Title|Comments\n---|---\n"

 #Build the table for the comment
        comment_table = comment_table_header
        for ama in amas_found:
            title = ama[0]
            if len(title) &gt; 75:
                title = title[:72] + "..."
            link = ama[1]
            n_comments = ama[2]
            comments_string = str(n_comments)+" Comments"
            comments_link = "["+comments_string+"]"+"("+link.encode('utf-8', 'ignore')+")"
            comment_table = comment_table + title + '|' + comments_link + '\n'

 #Build the body of the comment. Needs prettifying
        body = "User /u/"+str(parent.name) +" has posted the following AMAs:\n\n" + comment_table + "\n\n^(I'm just a bot made by /u/Molag_balls, If I've gotten something wrong, let him know)"
 
 #We don't want the script to break if we get a rate limit
        print 'Commenting...'
        try:
            comment.reply(body)
            already_done.append(comment.id)
        except Exception, e:
            print e
            continue


    already_done_to_db(already_done)
    already_done = []
    print "Sleeping for 5 minutes"
    time.sleep(180)

</pre>
The code above is commented, so I won't go through it line by line, but in broad strokes:

1) The script grabs all the most recent comments in a subreddit, and checks to see if they begin with "!identify", which I have chosen as my "summoning" phrase.

2) The script checks a MongoDB database to see whether or not we've handled this comment before. If not, it continues.

3) Then, if the comment starts with "!identify" the script will either: Use the parent comment's author and will check for their AMAs, or if a user is specified, then it will check for their AMAs

4) If any AMAs are found some sparse information is saved about them (Permalink, title, number of comments) and is formatted into a reddit table.

5) The bot comments, adds the summoning comment IDs to the Mongo database, and sleeps for 5 minutes (or however long you want).

The output looks a little something like this:

<img class="aligncenter  wp-image-227" src="http://manicfringe.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-26-at-4.12.42-PM.png" alt="Screen Shot 2015-09-26 at 4.12.42 PM" width="644" height="202" />

The code is pretty ugly, but it works okay from what I've seen and I did this on a lark, so I'm not terribly concerned. Now I have to figure out how to spread the word about the summoning phrase, so I can set it loose in the wild and see if people actually find it useful and so I can improve it if need be.
<h2 style="text-align: center;">Next Steps</h2>
1) Set this up to run continuously on an unused computer in my house, because I'm too cheap and lazy to find and set up a host online.

2) Make it so it deletes comments with -1 comment karma, so the reddit hivemind won't get angry.

3) Make more reddit bots because this was fun.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>214</wp:post_id>
		<wp:post_date>2015-09-26 15:24:09</wp:post_date>
		<wp:post_date_gmt>2015-09-26 23:24:09</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>weekend-project-reddit-ama-bot</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="tech"><![CDATA[Code]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_twitter_user</wp:meta_key>
			<wp:meta_value><![CDATA[@lgendrot]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_thumbnail_id</wp:meta_key>
			<wp:meta_value><![CDATA[217]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_done_all</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839229</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_wpas_skip_9839241</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>dsq_thread_id</wp:meta_key>
			<wp:meta_value><![CDATA[4169114642]]></wp:meta_value>
		</wp:postmeta>
	</item>
</channel>
</rss>
